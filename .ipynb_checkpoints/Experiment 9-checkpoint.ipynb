{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.utils.data import Path, pad_collate, grandparent_splitter\n",
    "from sentimentanalyser.utils.data import parent_labeler\n",
    "from sentimentanalyser.data.text import TextList, SplitData\n",
    "from sentimentanalyser.utils.files import pickle_dump, pickle_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.preprocessing.processor import TokenizerProcessor\n",
    "from sentimentanalyser.preprocessing.processor import NuemericalizeProcessor\n",
    "from sentimentanalyser.utils.data import read_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imdb = Path(\"/home/anukoolpurohit/Documents/AnukoolPurohit/Datasets/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_wiki = Path(\"/home/anukoolpurohit/Documents/AnukoolPurohit/Datasets/wikitext-103\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cache = Path('/home/anukoolpurohit/Documents/AnukoolPurohit/Models/WordEmbeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_eng = vocab.FastText(cache=path_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok = TokenizerProcessor()\n",
    "proc_num = NuemericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = TextList(read_wiki(path_wiki/'train.txt'), path_wiki)\n",
    "# valid = TextList(read_wiki(path_wiki/'valid.txt'), path_wiki)\n",
    "\n",
    "# len(train), len(valid)\n",
    "\n",
    "# sd_wiki = SplitData(train, valid)\n",
    "\n",
    "# lm_wiki = sd_wiki.label_by_func(lambda x:0, proc_x=[proc_tok, proc_num])\n",
    "\n",
    "# pickle_dump(lm_wiki, 'dumps/variable/ll_wiki.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_wiki = pickle_load('dumps/variable/ll_wiki.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 32, 70\n",
    "wiki_data = lm_wiki.lm_databunchify(bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = lm_wiki.train.proc_x[-1].vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = next(iter(wiki_data.train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.models.regularization import WeightDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.utils.dev import print_dims\n",
    "from sentimentanalyser.utils.models import get_info, get_embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_y(y):\n",
    "    if isinstance(y, (list, tuple)):\n",
    "        print(\"------------------\")\n",
    "        for yi in y:\n",
    "                display_y(yi)\n",
    "    else:\n",
    "        print_dims(\"tensor\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, inp):  \n",
    "        if not self.training or self.dropout == 0.:\n",
    "            return inp\n",
    "        bs , seq_len, vocab_size = inp.size()\n",
    "        mask = dropout_mask(inp.data, (bs, 1, vocab_size), self.dropout)\n",
    "        return inp * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithDropout(nn.Module):\n",
    "    def __init__(self, embeddings, embeddings_dropout):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.embeddings_dropout = embeddings_dropout\n",
    "        self.padding_idx = self.embeddings.padding_idx\n",
    "        if self.padding_idx is None:\n",
    "            self.padding_idx = -1\n",
    "    \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embeddings_dropout != 0:\n",
    "            vocab_length, embedding_size = self.embeddings.weight.size()\n",
    "            mask = dropout_mask(self.embeddings.weight.data,\n",
    "                                (vocab_length, 1),\n",
    "                                self.embeddings_dropout)\n",
    "            \n",
    "            masked_embeddings = self.embeddings.weight * mask\n",
    "            \n",
    "        else:\n",
    "            masked_embeddings = self.embeddings.weight\n",
    "        \n",
    "        return F.embedding(words, masked_embeddings, self.padding_idx,\n",
    "                           self.embeddings.max_norm, self.embeddings.norm_type,\n",
    "                           self.embeddings.scale_grad_by_freq, self.embeddings.sparse)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWDLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embdeding_size, hidden_size, num_layers, weight_drop=0.5,\n",
    "                 hidden_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embdeding_size, self.batch_size = embdeding_size, 1\n",
    "        self.rnns = []\n",
    "        self.hidden_dropouts = []\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            input_size = embdeding_size if layer == 0 else hidden_size\n",
    "            output_size = hidden_size if layer != num_layers else embdeding_size    \n",
    "            rnn = nn.LSTM(input_size, output_size, num_layers=1, batch_first=True)\n",
    "            self.rnns.append(WeightDropout(rnn, weight_drop))\n",
    "            self.hidden_dropouts.append(RNNDropout(hidden_drop))\n",
    "        \n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.hidden_dropouts = nn.ModuleList(self.hidden_dropouts)\n",
    "    \n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.hidden_size if l != self.num_layers - 1 else self.embdeding_size\n",
    "        return next(self.parameters()).new(1, self.batch_size, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l))\n",
    "                       for l in range(self.num_layers)]\n",
    "    \n",
    "    def forward(self, embedded):\n",
    "        batch_size, seq_len, vocab_size = embedded.size()\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "            self.reset()\n",
    "        \n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        \n",
    "        raw_output = embedded\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dropouts)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.num_layers - 1:\n",
    "                raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALTEncAWDLSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, padding_idx,\n",
    "                 hidden_drop=0.2, input_drop=0.6, embeddings_drop=0.1, weight_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
    "        self.embeddings_dropout = EmbeddingsWithDropout(self.embeddings, embeddings_drop)\n",
    "        \n",
    "        self.rnns = AWDLSTM(embedding_size, hidden_size, num_layers, weight_drop, hidden_drop)\n",
    "        \n",
    "        self.embeddings.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        self.input_dropout = RNNDropout(input_drop)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        embedded = self.input_dropout(self.embeddings_dropout(texts))\n",
    "        raw_outputs, outputs = self.rnns(embedded)\n",
    "        return raw_outputs, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncAWDLSTMWithEmbeds(nn.Module):\n",
    "    initrange=0.1\n",
    "    def __init__(self, local_vocab, torchtext_vocab, hidden_size, num_layers, padding_idx,\n",
    "                 hidden_drop=0.2, input_drop=0.6, embeddings_drop=0.1, weight_drop=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        embd_vecs = get_embedding_vectors(local_vocab, torchtext_vocab)\n",
    "        embedding_size = embd_vecs.shape[-1]\n",
    "        print(padding_idx)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embd_vecs,\n",
    "                                                       freeze=false,\n",
    "                                                       padding_idx=padding_idx)\n",
    "        \n",
    "        self.embeddings_dropout = EmbeddingsWithDropout(self.embeddings, embeddings_drop)\n",
    "        \n",
    "        self.rnns = AWDLSTM(embedding_size, hidden_size, num_layers, weight_drop, hidden_drop)\n",
    "        \n",
    "        #self.embeddings.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        self.input_dropout = RNNDropout(input_drop)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        embedded = self.input_dropout(self.embeddings_dropout(texts))\n",
    "        raw_outputs, outputs = self.rnns(embedded)\n",
    "        return raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, hidden_sz, output_sz, dropout, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(dropout)\n",
    "        self.decoder = nn.Linear(hidden_sz, output_sz, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: nn.init.kaiming_uniform_(self.decoder.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        #output = self.output_dp(outputs)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, outputs, raw_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMClassifierModel(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_sz=300, hidden_sz=300, output_sz=1, dropout=0.2,\n",
    "                 pad_idx=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.dps = dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * dropout\n",
    "        self.encoder = ALTEncAWDLSTM(vocab_sz, emb_sz, hidden_sz, num_layers, pad_idx,\n",
    "                                      *dps[:-1])\n",
    "        self.decoder = LinearDecoder(hidden_sz, vocab_sz, dps[-1], tie_encoder=self.encoder.embeddings)\n",
    "        return\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        output_enc = self.encoder(xb)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return output_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMClassifierModelWithEmbeds(nn.Module):\n",
    "    def __init__(self, local_vocab, torchtext_vocab, hidden_sz=300, output_sz=1, dropout=0.2,\n",
    "                 pad_idx=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.dps = dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * dropout\n",
    "        self.encoder = En(local_vocab, torchtext_vocab, hidden_sz, num_layers, pad_idx,\n",
    "                                      *dps[:-1])\n",
    "        self.decoder = LinearDecoder(hidden_sz, vocab_sz, dps[-1], tie_encoder=self.encoder.embeddings)\n",
    "        return\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        output_enc = self.encoder(xb)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return output_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMClassifierModel(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_main = model(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "size of tensor is torch.Size([32, 70, 60001])\n",
      "------------------\n",
      "size of tensor is torch.Size([32, 70, 300])\n",
      "size of tensor is torch.Size([32, 70, 300])\n",
      "------------------\n",
      "size of tensor is torch.Size([32, 70, 300])\n",
      "size of tensor is torch.Size([32, 70, 300])\n"
     ]
    }
   ],
   "source": [
    "display_y(y_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and accuracy flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.callbacks.core import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCustomTrainer(Callback):\n",
    "    def __init__(self, α, β):\n",
    "        self.α = α\n",
    "        self.β = β\n",
    "        \n",
    "    def after_pred(self):\n",
    "        self.out, self.raw_out = self.preds[1], self.preds[2]\n",
    "        self.trainer.preds = self.trainer.preds[0]\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if self.α != 0.:\n",
    "            self.trainer.loss += self.α * self.out[-1].float().pow(2).mean()\n",
    "        \n",
    "        if self.β != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h) > 1:\n",
    "                self.trainer.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "    def begin_epoch(self):\n",
    "        if hasattr(self.dl.dataset, \"batchify\"):\n",
    "            self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.optimizers import adam_opt\n",
    "from sentimentanalyser.utils.metrics import accuracy\n",
    "from sentimentanalyser.utils.callbacks import combine_scheds, sched_cos, cos_1cycle_anneal\n",
    "from sentimentanalyser.utils.callbacks import create_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic(Model, vocab):\n",
    "    model = Model(len(vocab))\n",
    "    loss_func = cross_entropy_flat\n",
    "    opt = adam_opt()(model.parameters())\n",
    "    return model, loss_func, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(Model, local_vocab, torchtext_vocab):\n",
    "    model = Model(local_vocab, torchtext_vocab)\n",
    "    loss_func = cross_entropy_flat\n",
    "    opt = adam_opt()(model.parameters())\n",
    "    return model, loss_func, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-4, 1e-3), sched_cos(1e-3, 3e-5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.utils.callbacks import sched_cos, combine_scheds\n",
    "from sentimentanalyser.callbacks.training import LRFind, CudaCallback, GradientClipping\n",
    "from sentimentanalyser.callbacks.progress import ProgressCallback\n",
    "from sentimentanalyser.callbacks.scheduler import ParamSchedulerCustom\n",
    "from sentimentanalyser.callbacks.stats import AvgStatsCallback\n",
    "from sentimentanalyser.callbacks.recorder import RecorderCustom\n",
    "from sentimentanalyser.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback, [accuracy_flat]),\n",
    "       partial(ParamSchedulerCustom,'lr', [sched]),\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       ProgressCallback,\n",
    "       CudaCallback,\n",
    "       partial(RNNCustomTrainer, α=2., β=1.),\n",
    "       RecorderCustom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ecda0c78df3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mget_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLMClassifierModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_eng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_funcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-f2e5497699bd>\u001b[0m in \u001b[0;36mget_embeds\u001b[0;34m(Model, local_vocab, torchtext_vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchtext_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchtext_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_flat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-13203cc97998>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_sz, emb_sz, hidden_sz, output_sz, dropout, pad_idx, num_layers)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         self.encoder = ALTEncAWDLSTM(vocab_sz, emb_sz, hidden_sz, num_layers, pad_idx,\n\u001b[0;32m----> 7\u001b[0;31m                                       *dps[:-1])\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtie_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-037aeed9baac>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_size, hidden_size, num_layers, padding_idx, hidden_drop, input_drop, embeddings_drop, weight_drop)\u001b[0m\n\u001b[1;32m      4\u001b[0m                  hidden_drop=0.2, input_drop=0.6, embeddings_drop=0.1, weight_drop=0.5):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingsWithDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai2/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding_idx must be within num_embeddings'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mpadding_idx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding_idx must be within num_embeddings'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(wiki_data, *get_embeds(LMClassifierModel, vocab, ft_eng), cb_funcs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
