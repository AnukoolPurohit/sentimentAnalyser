{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sentimentanalyser.data.text import TextList, ItemList, DataBunch, SplitData\n",
    "from sentimentanalyser.utils.data import Path, listify, random_splitter, compose, parallel, pad_collate, parent_labeler, read_wiki, grandparent_splitter\n",
    "from sentimentanalyser.data.samplers import SortishSampler, SortSampler\n",
    "from sentimentanalyser.utils.preprocessing import *\n",
    "from sentimentanalyser.utils.files import pickle_dump, pickle_load\n",
    "from sentimentanalyser.preprocessing.processor import TokenizerProcessor, NuemericalizeProcessor, CategoryProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imdb = Path(\"/home/anukoolpurohit/Documents/AnukoolPurohit/Datasets/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok = TokenizerProcessor()\n",
    "proc_num = NuemericalizeProcessor()\n",
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_imdb = TextList.from_files(path_imdb, folders=['train', 'test'])\n",
    "sd_imdb = il_imdb.split_by_func(partial(grandparent_splitter, valid_name='test'))\n",
    "ll_imdb = sd_imdb.label_by_func(parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(ll_imdb, 'dumps/variable/ll_imdb.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_imdb = pickle_load('dumps/variable/ll_imdb.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = ll_imdb.clas_databunchify(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimentanalyser.utils.callbacks import sched_cos, combine_scheds\n",
    "from sentimentanalyser.callbacks.training import LR_Find, CudaCallback\n",
    "from sentimentanalyser.callbacks.progress import ProgressCallback\n",
    "from sentimentanalyser.callbacks.scheduler import ParamScheduler\n",
    "from sentimentanalyser.callbacks.stats import AvgStatsCallback\n",
    "from sentimentanalyser.callbacks.recorder import Recorder\n",
    "from sentimentanalyser.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    preds   = torch.argmax(preds, dim=1)\n",
    "    correct = (preds == y).float()\n",
    "    acc     = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lens_and_masks(x, pad_id=1):\n",
    "    mask = (x == pad_id)\n",
    "    lenghts = x.size(1) - (x == pad_id).sum(1)\n",
    "    return lenghts, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model0(nn.Module):\n",
    "    def __init__(self, vocab_size=proc_num.vocab_size, num_layers=2,\n",
    "                 hidden_size=50, output_size=2, bidirectional=True,\n",
    "                 padding_idx=1, bs=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size, self.hidden_size, self.output_size   = vocab_size, hidden_size, output_size\n",
    "        self.num_layers, self.bidirectional, self.padding_idx = num_layers, bidirectional, padding_idx\n",
    "        self.bidir = 2 if bidirectional is True else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size, padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.dropout   = nn.Dropout()\n",
    "        \n",
    "        self.rnn       = nn.LSTM(self.hidden_size, self.hidden_size,\n",
    "                                 num_layers=self.num_layers,\n",
    "                                 batch_first=True,\n",
    "                                 bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.fc        = nn.Linear(self.hidden_size * (self.bidir * self.num_layers), self.output_size)\n",
    "        return\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        text_lengths, _ = get_lens_and_masks(texts)\n",
    "        embeded = self.dropout(self.embedding(texts))\n",
    "        packed_embed = nn.utils.rnn.pack_padded_sequence(embeded, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embed)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat([h for h in hidden], dim=1))\n",
    "        linear = self.fc(hidden)\n",
    "        return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = next(iter(imdb_data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method=None, hidden_size=None):\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        return\n",
    "    \n",
    "    def dot_score(self, hidden, outputs):\n",
    "        shape = hidden.shape\n",
    "        hidden = hidden.view(shape[0],shape[2],shape[1]*shape[3])\n",
    "        hidden = torch.sum(hidden, dim=0)/shape[0]\n",
    "        return torch.sum(hidden * outputs, dim =2)\n",
    "    \n",
    "    def forward(self, hidden, outputs, mask):\n",
    "        attn_energies = self.dot_score(hidden, outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        attn_energies = F.softmax(attn_energies, dim=1)\n",
    "        return attn_energies.masked_fill(mask, 1e-9).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, vocab_size=proc_num.vocab_size, num_layers=2,\n",
    "                 hidden_size=50, output_size=2, bidirectional=True,\n",
    "                 padding_idx=1, bs=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size, self.hidden_size, self.output_size   = vocab_size, hidden_size, output_size\n",
    "        self.num_layers, self.bidirectional, self.padding_idx = num_layers, bidirectional, padding_idx\n",
    "        self.bidir = 2 if bidirectional is True else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size, padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.dropout   = nn.Dropout()\n",
    "        \n",
    "        self.rnn       = nn.LSTM(self.hidden_size, self.hidden_size,\n",
    "                                 num_layers=self.num_layers,\n",
    "                                 batch_first=True,\n",
    "                                 bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.concat    = nn.Linear(self.hidden_size * self.bidir, self.hidden_size)\n",
    "        self.fc        = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.att       = Attn()\n",
    "        return\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        text_lengths, mask = get_lens_and_masks(texts)\n",
    "        embeded = self.dropout(self.embedding(texts))\n",
    "        packed_embed = nn.utils.rnn.pack_padded_sequence(embeded, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embed)\n",
    "        \n",
    "        \n",
    "        outputs, lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        att_energies = self.att(hidden.view(self.num_layers, self.bidir, texts.shape[0], self.hidden_size), outputs, mask)\n",
    "        context = att_energies.bmm(outputs.transpose(0,1))\n",
    "        context = context.squeeze(1)\n",
    "        hidden  = torch.cat([h for h in hidden], dim=1)\n",
    "        final   = torch.cat([hidden, context],dim=-1)\n",
    "        context = self.concat(context)\n",
    "        context = torch.tanh(context)\n",
    "        linear  = self.fc(self.dropout(context))\n",
    "        return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dims(name, tensor):\n",
    "    print(f'size of {name} is {tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn2(nn.Module):\n",
    "    def __init__(self, method=None, hidden_size=None, context_size=None):\n",
    "        super().__init__()\n",
    "        self.method, self.hidden_size, self.context_size = method, hidden_size, context_size\n",
    "        \n",
    "        self.fc     = nn.Linear(self.hidden_size, self.context_size)\n",
    "        self.weight = nn.Parameter(torch.randn(self.context_size))\n",
    "    \n",
    "    def forward(self, enc, mask):\n",
    "        attn_energies = torch.tanh(self.fc(enc))\n",
    "        weights = torch.softmax(attn_energies.matmul(self.weight), dim=1)\n",
    "        #weights = weights.masked_fill(mask, 1e-9)\n",
    "        weights = weights.unsqueeze(1)\n",
    "        res = torch.sum(weights.matmul(enc), dim=1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, vocab_size=proc_num.vocab_size, hidden_size=50,\n",
    "                 context_size=50, output_size=2, num_layers=2,\n",
    "                 bidirectional=True, padding_idx=1, bs=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.hidden_size       = vocab_size, hidden_size\n",
    "        self.context_size, self.output_size     = context_size, output_size\n",
    "        self.num_layers, self.bidirectional     = num_layers, bidirectional\n",
    "        self.padding_idx, self.bs, self.dropout = padding_idx, bs, dropout\n",
    "        \n",
    "        self.bidir = 2 if self.bidirectional else 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size,\n",
    "                                      padding_idx=self.padding_idx)\n",
    "        self.dropout   = nn.Dropout()\n",
    "        self.rnn       = nn.LSTM(self.hidden_size, self.hidden_size,\n",
    "                                 num_layers=self.num_layers,\n",
    "                                 batch_first=True,\n",
    "                                 bidirectional=self.bidirectional)\n",
    "        self.attn      = Attn2('3', self.bidir*self.hidden_size, self.context_size)\n",
    "        self.fc        = nn.Linear(self.bidir*self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        text_lengths, mask = get_lens_and_masks(texts, pad_id=self.padding_idx)\n",
    "        \n",
    "        embedded       = self.embedding(texts)\n",
    "        packed_embeded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,\n",
    "                                                           batch_first=True)\n",
    "        enc_packed, (hidden, cell) = self.rnn(packed_embeded)\n",
    "        \n",
    "        enc,_ = nn.utils.rnn.pad_packed_sequence(enc_packed, batch_first=True)\n",
    "        res   = self.attn(enc, mask)\n",
    "        return self.fc(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model corollary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=proc_num.vocab_size, embed_d=50, hidden_d=50, output_d=2, context_d=50,\n",
    "                 dropout=0.5, pad_idx=1, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_d,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(bidirectional=True, num_layers=num_layers,\n",
    "                            input_size=embed_d, hidden_size=hidden_d,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "\n",
    "        ## Word-level hierarchical attention:\n",
    "        self.ui = nn.Linear(2*hidden_d, context_d)\n",
    "        self.uw = nn.Parameter(torch.randn(context_d))\n",
    "\n",
    "        ## Output:\n",
    "        self.fc = nn.Linear(2*hidden_d, output_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seqlens, mask = get_lens_and_masks(x)\n",
    "\n",
    "        embeds = self.embedding(x) # B X T X EmbD\n",
    "        packed_embeds = nn.utils.rnn.pack_padded_sequence(embeds, seqlens,\n",
    "                                                          batch_first=True)\n",
    "            # 960 (B*T) X 300 (N) B*T X EmbD\n",
    "\n",
    "        enc_packed, (h_n, c_n) = self.lstm(packed_embeds)\n",
    "            # (B*T) X HdD*2\n",
    "        enc, _ = nn.utils.rnn.pad_packed_sequence(enc_packed,\n",
    "                                                  batch_first=True)\n",
    "            # B X T X HdD*2\n",
    "\n",
    "        ## Word-level hierarchical attention:\n",
    "        u_it = torch.tanh(self.ui(enc)) # B X T X CtD\n",
    "        weights = torch.softmax(u_it.matmul(self.uw), dim=1).unsqueeze(1)\n",
    "            # B X 1 X T\n",
    "        sent = torch.sum(weights.matmul(enc), dim=1) # B X HdD*2\n",
    "\n",
    "        logits = self.fc(sent) # B X OutD\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic0():\n",
    "    model = Model0(num_layers=4)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_func, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic1():\n",
    "    model = Model1(num_layers=4)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_func, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic2():\n",
    "    model = Model2(num_layers=4)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, loss_func, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.3, 0.7], [sched_cos(3e-3, 1e-2), sched_cos(1e-2, 3e-5)])\n",
    "sched_fast = combine_scheds([0.3, 0.7], [sched_cos(3e-2, 5e-1), sched_cos(5e-1, 1e-3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer0 = Trainer(imdb_data, *get_basic0(), cb_funcs=[\n",
    "    partial(AvgStatsCallback, [accuracy]),\n",
    "    partial(ParamScheduler,'lr', sched),\n",
    "    ProgressCallback,\n",
    "    CudaCallback,\n",
    "    Recorder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = Trainer(imdb_data, *get_basic1(), cb_funcs=[\n",
    "    partial(AvgStatsCallback, [accuracy]),\n",
    "    partial(ParamScheduler,'lr', sched),\n",
    "    ProgressCallback,\n",
    "    CudaCallback,\n",
    "    Recorder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(imdb_data, *get_basic2(), cb_funcs=[\n",
    "    partial(AvgStatsCallback, [accuracy]),\n",
    "    partial(ParamScheduler,'lr', sched),\n",
    "    ProgressCallback,\n",
    "    CudaCallback,\n",
    "    Recorder\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer0.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer2.fit(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
